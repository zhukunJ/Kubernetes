# Kubernetes高可用性集群部署


## Kubernetes版本选择

Kubernetes 1.14是2019年发布的第一个正式版本。新版本有31个增强功能：其中有10个功能进入了生产可用状态，12个进入了beta版本，另外，增加了7个新功能。1.14版本的主题是可扩展性，支持更多Kubernetes工作负载，其中三个主要功能正式推出，以及一个重要的安全功能转向beta。与之前发布的Kubernetes版本相比，1.14版本中的更多功能逐渐稳定，这是大家期望的重要里程碑。

- **生产级别的Windwos节点支持** Kubernetes1.14版本正式支持将Windows节点添加为工作节点，这意味着可以在生产环境使用Windows容器了，使庞大的Windows应用生态系统能够利用Kubernetes平台的强大功能了。
- **Kubectl的更新** 新版本Kubectl可以使用声明性Resource Config来管理资源。kubectl的文档也从头进行编写，可参考[文档](https://kubectl.docs.kubernetes.io)。
- **持久化本地Volumes** 本地SSD比远程磁盘能提供更好的性能。对性能要求比较高的数据库和分布式文件系统可以使用久化本地存储。

## 准备节点

通过kubeadm部署高可用Kubernetes集群有两种架构，一种是将数据平面（etcd集群）和控制平面（Kubernetes控制节点）部署在一起，另一种是分开部署，其中部署在一起可以节省服务器，但是数据平面和控制平面耦合在一起，当一台机器故障时，数据平面和控制平面将同时出现问题。

数据平面和控制平面共用节点：

![数据平面和控制平面共用节点](https://github.com/findsec-cn/k201/raw/master/imgs/1/kubeadm-ha-topology-stacked-etcd.png)

数据平面和控制平面不共用节点：

![数据平面和控制平面不共用节点](https://github.com/findsec-cn/k201/raw/master/imgs/1/kubeadm-ha-topology-external-etcd.png)

我们按照数据平面和控制平面共用节点进行高可用集群的部署。

我们可以通过[Terraform项目](https://github.com/findsec-cn/terraform)在阿里云一键生成Kubernetes集群所需资源。

## 系统配置

### 升级各节点系统

操作系统我们选择 CentOS 7 最新版（7.6.1810），如果不是最新版，可参考如下升级到最新版。

按如下内容，编辑 /etc/yum.repos.d/CentOS-Base.repo

    # CentOS-Base.repo
    #
    # The mirror system uses the connecting IP address of the client and the
    # update status of each mirror to pick mirrors that are updated to and
    # geographically close to the client.  You should use this for CentOS updates
    # unless you are manually picking other mirrors.
    #
    # If the mirrorlist= does not work for you, as a fall back you can try the
    # remarked out baseurl= line instead.
    #
    #

    [base]
    name=CentOS-$releasever - Base
    #mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os&infra=$infra
    baseurl=http://mirrors.163.com/centos/7.6.1810/os/$basearch/
    #baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/
    gpgcheck=1
    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

    #released updates
    [updates]
    name=CentOS-$releasever - Updates
    #mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates&infra=$infra
    baseurl=http://mirrors.163.com/centos/7.6.1810/updates/$basearch/
    #baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/
    gpgcheck=1
    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

    #additional packages that may be useful
    [extras]
    name=CentOS-$releasever - Extras
    #mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras&infra=$infra
    baseurl=http://mirrors.163.com/centos/7.6.1810/extras/$basearch/
    #baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/
    gpgcheck=1
    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

    #additional packages that extend functionality of existing packages
    [centosplus]
    name=CentOS-$releasever - Plus
    #mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus&infra=$infra
    baseurl=http://mirrors.163.com/centos/7.6.1810/centosplus/$basearch/
    #baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/
    gpgcheck=1
    enabled=0
    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

升级系统并重启

    $ yum update -y
    $ reboot

### 设置主机名

    hostnamectl set-hostname k8s-m001
    hostnamectl set-hostname k8s-m002
    hostnamectl set-hostname k8s-m003
    hostnamectl set-hostname k8s-s001
    hostnamectl set-hostname k8s-s002
    hostnamectl set-hostname k8s-s003

### 配置hosts

    172.16.10.48 k8s-m001
    172.16.10.49 k8s-m002
    172.16.10.50 k8s-m003
    172.16.10.51 k8s-s001
    172.16.10.52 k8s-s002
    172.16.10.53 k8s-s003

### 关闭SELinux

关闭SELinux，设置/etc/sysconfig/selinux，配置为SELINUX=disabled

    $ sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
    $ setenforce 0

### 关闭防火墙

    $ systemctl stop firewalld
    $ systemctl disable firewalld

### 关闭Swap分区

Kubernetes v1.8+要求关闭系统 Swap：

    $ vi /etc/fstab
    注释swap相关的行
    $ swapoff -a && sysctl -w vm.swappiness=0

### 配置内核参数

    $ cat <<EOF >  /etc/sysctl.d/k8s.conf
    net.bridge.bridge-nf-call-ip6tables = 1
    net.bridge.bridge-nf-call-iptables = 1
    net.ipv4.ip_forward = 1
    vm.swappiness = 0
    EOF
    $ sysctl --system

### 加载ipvs相关模块

kube-proxy使用ipvs模式，所以需要加ipvs相关的内核模块及安装ipset、ipvsadm软件包。

加载相关内核模块

    $ cat > /etc/sysconfig/modules/ipvs.modules << EOF
    #!/bin/bash
    modprobe -- ip_vs
    modprobe -- ip_vs_rr
    modprobe -- ip_vs_wrr
    modprobe -- ip_vs_sh
    modprobe -- nf_conntrack_ipv4
    EOF

    $ chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4

安装相关软件包

    $ yum install -y ipset ipvsadm

### 配置ssh key认证

在主节点1上执行生成公钥

    $ ssh-keygen

拷贝 /root/.ssh/id_rsa.pub 内容到主节点2和主节点3上

    $ vi /home/centos/.ssh/authorized_keys

## 安装 Docker + 安装依赖包 + 启动

    $ cat >> docker-ce-install.sh << EOF
      #!/bin/bash
      set -e
      echo "# 卸载旧版本"
      sudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate  docker-logrotate docker-selinux    docker-engine-selinux  docker-engine -y
      echo "# 安装依赖包"
      sudo yum install -y yum-utils device-mapper-persistent-data lvm2
      echo "# 添加国内yum 源"
      sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
      echo "# 更新 yum 软件源缓存，并安装 docker-ce"
      sudo yum makecache fast
      sudo yum install docker-ce-18.09.0-3.el7 -y
      mkdir -p /etc/docker
      echo "#配置镜像加速,设置cgroup驱动使用systemd"
      cat << EOF > /etc/docker/daemon.json
      {
      "registry-mirrors": ["https://u9nigs6v.mirror.aliyuncs.com","http://hub-mirror.c.163.com","https://docker.mirrors.ustc.edu.cn","https://registry.docker-cn.com"],
      "graph": "/data/docker",
      "exec-opts": ["native.cgroupdriver=systemd"],
      "log-driver": "json-file",
      "log-opts": {
          "max-size": "100m"
      },
      "storage-driver": "overlay2",
      "storage-opts": [
          "overlay2.override_kernel_check=true"
      ]
      }
      EOF
      echo "#启动 Docker CE"
      sudo systemctl enable docker
      sudo systemctl start docker
      sudo systemctl status docker
      echo "#测试 Docker 是否安装正确"
      docker info


## 搭建高可用etcd集群
三台node节点上安装cfssl
    
    cd /usr/local/src
    wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
    wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
    wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
    chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64
    mv cfssl_linux-amd64 /usr/local/bin/cfssl
    mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
    mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo
    chmod +x /usr/bin/cfssl-certinfo

## 创建ca证书，客户端，服务端，节点之间的证书
Etcd属于server ,etcdctl 属于client，二者之间通过http协议进行通信。
ca证书 自己给自己签名的权威证书，用来给其他证书签名
server证书 etcd的证书
client证书 客户端，比如etcdctl的证书
peer证书 节点与节点之间通信的证书

    1）创建目录
    在node01上操作
    mkdir -p /etc/etcd/pki
    cd /etc/etcd/pki

    2）创建ca证书
    修改ca-config.json
    server auth表示client可以用该ca对server提供的证书进行验证
    client auth表示server可以用该ca对client提供的证书进行验证
    
    vim ca-config.json
    {
        "signing": {
            "default": {
                "expiry": "86400h"
            },
            "profiles": {
                "server": {
                    "expiry": "86400h",
                    "usages": [
                        "signing",
                        "key encipherment",
                        "server auth",
                        "client auth"
                    ]
                },
                "client": {
                    "expiry": "86400h",
                    "usages": [
                        "signing",
                        "key encipherment",
                        "client auth"
                    ]
                },
                "peer": {
                    "expiry": "86400h",
                    "usages": [
                        "signing",
                        "key encipherment",
                        "server auth",
                        "client auth"
                    ]
                }
            }
        }
    }

    创建证书签名请求ca-csr.json
        vim ca-csr.json
        {
            "CN": "etcd",
            "key": {
                "algo": "rsa",
                "size": 2048
            }
        }

    生成CA证书和私钥
    cfssl gencert -initca ca-csr.json | cfssljson -bare ca

    3） 生成客户端证书
        vim client.json
        {
            "CN": "client",
            "key": {
                "algo": "ecdsa",
                "size": 256
            }
        }

    生成
        cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json  | cfssljson -bare client -


    4） 生成server，peer证书
    创建配置etcd.json
    vim etcd.json
    {
        "CN": "etcd",
        "hosts": [
            "127.0.0.1",
            "10.0.1.207", #master1
            "10.0.1.208", #master2
            "10.0.1.209" #master3
        ],
        "key": {
            "algo": "ecdsa",
            "size": 256
        },
        "names": [
            {
                "C": "CN",
                "L": "BJ",
                "ST": "BJ"
            }
        ]
    }

    生成
        cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server etcd.json | cfssljson -bare server
        cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer etcd.json | cfssljson -bare peer


    5)在master02,master03分别创建目录
        mkdir -p /etc/etcd/pki/
    6)将 master01的/etc/etcd/pki/目录同步到master02,master03上
        scp -r * root@k8s-test002:/etc/etcd/pki/
        scp -r * root@k8s-test003:/etc/etcd/pki/

## 安装etcd二进制文件

    在三台master上安装
    需要注意的是你安装的版本必须大于等于版本3.2.18，否则报错如下：
        [ERROR ExternalEtcdVersion]: this version of kubeadm only supports external etcd version >= 3.2.18. Current version: 3.1.5

    下载二进制文件，并将解压后的两个命令移动到/usr/local/bin目录下
        cd /usr/local/src
        wget https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz
        tar -xvf etcd-v3.3.10-linux-amd64.tar.gz
        mv etcd-v3.3.10-linux-amd64/etcd* /usr/local/bin



    增加service配置文件
    三台机器配置不一样，需要替换为相应的IP和name,注意格式很重要，后面的\与结尾的字母中间只能相隔一个空格。
    master01上操作：
    vim /usr/lib/systemd/system/etcd.service
        [Unit]
        Description=Etcd Server
        After=network.target
        After=network-online.target
        Wants=network-online.target
        Documentation=https://github.com/coreos

        [Service]
        Type=notify
        WorkingDirectory=/var/lib/etcd
        ExecStart=/usr/local/bin/etcd \
        --data-dir=/var/lib/etcd  \
        --name=etcd01 \
        --cert-file=/etc/etcd/pki/server.pem \
        --key-file=/etc/etcd/pki/server-key.pem \
        --trusted-ca-file=/etc/etcd/pki/ca.pem \
        --peer-cert-file=/etc/etcd/pki/peer.pem \
        --peer-key-file=/etc/etcd/pki/peer-key.pem \
        --peer-trusted-ca-file=/etc/etcd/pki/ca.pem \
        --listen-peer-urls=https://10.0.1.207:2380 \
        --initial-advertise-peer-urls=https://10.0.1.207:2380 \
        --listen-client-urls=https://10.0.1.207:2379,http://127.0.0.1:2379 \
        --advertise-client-urls=https://10.0.1.207:2379 \
        --initial-cluster-token=etcd-cluster-0 \
        --initial-cluster=etcd01=https://10.0.1.207:2380,etcd02=https://10.0.1.208:2380,etcd03=https://10.0.1.209:2380 \
        --initial-cluster-state=new \
        --heartbeat-interval=250 \
        --election-timeout=2000
        Restart=on-failure
        RestartSec=5
        LimitNOFILE=65536

        [Install]
        WantedBy=multi-user.target

    master02上操作：
    vim /usr/lib/systemd/system/etcd.service   

        [Unit]
        Description=Etcd Server
        After=network.target
        After=network-online.target
        Wants=network-online.target
        Documentation=https://github.com/coreos

        [Service]
        Type=notify
        WorkingDirectory=/var/lib/etcd
        ExecStart=/usr/local/bin/etcd \
        --data-dir=/var/lib/etcd  \
        --name=etcd02 \
        --cert-file=/etc/etcd/pki/server.pem \
        --key-file=/etc/etcd/pki/server-key.pem \
        --trusted-ca-file=/etc/etcd/pki/ca.pem \
        --peer-cert-file=/etc/etcd/pki/peer.pem \
        --peer-key-file=/etc/etcd/pki/peer-key.pem \
        --peer-trusted-ca-file=/etc/etcd/pki/ca.pem \
        --listen-peer-urls=https://10.0.1.208:2380 \
        --initial-advertise-peer-urls=https://10.0.1.208:2380 \
        --listen-client-urls=https://10.0.1.208:2379,http://127.0.0.1:2379 \
        --advertise-client-urls=https://10.0.1.208:2379 \
        --initial-cluster-token=etcd-cluster-0 \
        --initial-cluster=etcd01=https://10.0.1.207:2380,etcd02=https://10.0.1.208:2380,etcd03=https://10.0.1.209:2380 \
        --initial-cluster-state=new \
        --heartbeat-interval=250 \
        --election-timeout=2000
        Restart=on-failure
        RestartSec=5
        LimitNOFILE=65536

        [Install]
        WantedBy=multi-user.target

    master03上操作：
    vim /usr/lib/systemd/system/etcd.service

        [Unit]
        Description=Etcd Server
        After=network.target
        After=network-online.target
        Wants=network-online.target
        Documentation=https://github.com/coreos

        [Service]
        Type=notify
        WorkingDirectory=/var/lib/etcd
        ExecStart=/usr/local/bin/etcd \
        --data-dir=/var/lib/etcd  \
        --name=etcd03 \
        --cert-file=/etc/etcd/pki/server.pem \
        --key-file=/etc/etcd/pki/server-key.pem \
        --trusted-ca-file=/etc/etcd/pki/ca.pem \
        --peer-cert-file=/etc/etcd/pki/peer.pem \
        --peer-key-file=/etc/etcd/pki/peer-key.pem \
        --peer-trusted-ca-file=/etc/etcd/pki/ca.pem \
        --listen-peer-urls=https://10.0.1.209:2380 \
        --initial-advertise-peer-urls=https://10.0.1.209:2380 \
        --listen-client-urls=https://10.0.1.209:2379,http://127.0.0.1:2379 \
        --advertise-client-urls=https://10.0.1.209:2379 \
        --initial-cluster-token=etcd-cluster-0 \
        --initial-cluster=etcd01=https://10.0.1.207:2380,etcd02=https://10.0.1.208:2380,etcd03=https://10.0.1.209:2380 \
        --initial-cluster-state=new \
        --heartbeat-interval=250 \
        --election-timeout=2000
        Restart=on-failure
        RestartSec=5
        LimitNOFILE=65536

        [Install]
        WantedBy=multi-user.target



    备注:
    * name 节点名称
    * data-dir 数据目录
    * listen-peer-urls 集群通信监听地址
    * listen-client-urls 客户端访问监听地址
    * initial-advertise-peer-urls 集群通告地址
    * advertise-client-urls 客户端通告地址
    * initial-cluster 集群节点地址
    * initial-cluster-token 集群Token
    * initial-cluster-state 加入集群的当前状态，new是新集群，existing表示加入



    在三台node上创建存放etcd数据的目录，启动 etcd
        mkdir /var/lib/etcd
        systemctl daemon-reload && systemctl enable etcd && systemctl start etcd && systemctl status etcd



    验证是否成功
    在任意一台机器（无论是不是集群节点，前提是需要有etcdctl工具和ca证书，server证书）上执行如下命令：
    cd /etc/etcd/pki/
    etcdctl --ca-file=/etc/etcd/pki/ca.pem --cert-file=/etc/etcd/pki/server.pem --key-file=/etc/etcd/pki/server-key.pem --endpoints="https://10.0.1.207:2379,https://10.0.1.208:2379,https://10.0.1.209:2379" cluster-health

    member bfe6a989862edec3 is healthy: got healthy result from https://10.0.1.209:2379
    member ce285c7771e7aafa is healthy: got healthy result from https://10.0.1.208:2379
    member eaa87eb6a2a382b8 is healthy: got healthy result from https://10.0.1.207:2379
    cluster is healthy


至此高可用ssl安全通信etcd集群搭建完成。



## 安装kubeadm, kubelet和kubectl

所有节点安装kubeadm, kubelet和kubectl，kubelet版本要与待安装的Kubernetes版本相同，否则可能会出现一些难以预料的问题。
安装kubelet，指定版本安装(需要在每台机器上都安装以下的软件包)
kubeadm: 用来初始化集群的指令。
kubelet: 在集群中的每个节点上用来启动 pod 和 container 等。
kubectl: 用来与集群通信的命令行工具。

### 配置yum仓库

    cat <<EOF > /etc/yum.repos.d/kubernetes.repo
    [kubernetes]
    name=Kubernetes
    baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
    enabled=1
    gpgcheck=0
    repo_gpgcheck=0
    gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
    EOF

### 更新缓存

    yum makecache fast

### kubelet版本列表

    yum list kubelet --showduplicates | sort -r 

    指定版本安装kubelet 1.14.6,#禁用除kubernetes之外的仓库

        yum -y install kubelet-1.14.6-0.x86_64   --disableexcludes=kubernetes
        systemctl enable kubelet 
        systemctl start kubelet


## 查看kubeadm和kubectl版本列表
    yum list kubeadm --showduplicates | sort -r 
    yum list kubectl --showduplicates | sort -r 

    #指定版本安装kubeadm-1.14.6-0,kubectl-1.14.6-0禁用除kubernetes之外的仓库
        yum -y install   kubeadm-1.14.6-0.x86_64  kubectl-1.14.6-0.x86_64 --disableexcludes=kubernetes

## 重启服务器
    为了防止发生某些未知错误，这里我们重启下服务器，方便进行后续操作 （也可不必重启）
    reboot



### 下载k8s集群构建所有需要用到的镜像
    在所有节点上提前下载以下镜像
    基础组件镜像
    网络插件镜像
    dashborad镜像（如果你准备安装）
    1） 先查看要用到的镜像有哪些，这里要注意的是：要拉取的4个核心组件的镜像版本和你安装的kubelet、kubeadm、kubectl 版本需要是一致的。
        $ kubeadm config images list     出现以下提示
            I0927 11:20:03.297921   10300 version.go:96] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt": Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
            I0927 11:20:03.298249   10300 version.go:97] falling back to the local client version: v1.14.6
            k8s.gcr.io/kube-apiserver:v1.14.6
            k8s.gcr.io/kube-controller-manager:v1.14.6
            k8s.gcr.io/kube-scheduler:v1.14.6
            k8s.gcr.io/kube-proxy:v1.14.6
            k8s.gcr.io/pause:3.1
            k8s.gcr.io/etcd:3.3.10
            k8s.gcr.io/coredns:1.3.1

    阿里同名镜像下载地址：
        docker pull registry.aliyuncs.com/google_containers/kube-proxy:v1.14.6
        docker pull registry.aliyuncs.com/google_containers/kube-apiserver:v1.14.6
        docker pull registry.aliyuncs.com/google_containers/kube-scheduler:v1.14.6
        docker pull registry.aliyuncs.com/google_containers/kube-controller-manager:v1.14.6
        docker pull registry.aliyuncs.com/google_containers/coredns:1.3.1
        docker pull registry.aliyuncs.com/google_containers/pause:3.1
        docker pull registry.aliyuncs.com/google_containers/etcd:3.3.10



### 初始化master01
    创建配置文件 kubeadm-config.yaml：
    vim  kubeadm-config.yaml

        apiVersion: kubeproxy.config.k8s.io/v1alpha1
        kind: KubeProxyConfiguration
        mode: "ipvs"
        ---
        apiVersion: kubeadm.k8s.io/v1beta1
        kind: ClusterConfiguration
        kubernetesVersion: v1.14.6   # 指定1.14.6版本
        apiServer:
            certSANs:
            - "10.0.1.207"
            - "10.0.1.208"
            - "10.0.1.209"
            - "127.0.0.1"
            extraArgs:
                allow-privileged: "true"
                feature-gates: "VolumeSnapshotDataSource=true,CSINodeInfo=true,CSIDriverRegistry=true"
        controlPlaneEndpoint: "123.57.222.14:6443"  # vip或者slb

        etcd:
        external:
            endpoints:
            - "https://10.0.1.207:2379"    #外部etcd地址,node01上部署
            - "https://10.0.1.208:2379"   #外部etcd地址,node02上部署
            - "https://10.0.1.209:2379"   #外部etcd地址,node03上部署
            caFile: /etc/etcd/pki/ca.pem  #搭建etcd集群时生成的ca证书
            certFile: /etc/etcd/pki/client.pem   #搭建etcd集群时生成的客户端证书
            keyFile: /etc/etcd/pki/client-key.pem  #搭建etcd集群时生成的客户端密钥

        networking:
            podSubnet: "10.244.0.0/16"  # 计划使用canal网络插件，指定pod网段及掩码
        controllerManager:
            extraArgs:
                address: 0.0.0.0
        scheduler:
            extraArgs:
                address: 0.0.0.0
        imageRepository: registry.aliyuncs.com/google_containers   # 指定镜像源为阿里源



### 初始化第一个节点
    kubeadm init --config=kubeadm-config.yaml    #出现以下返回


        [addons] Applied essential addon: kube-proxy

        Your Kubernetes control-plane has initialized successfully!

        To start using your cluster, you need to run the following as a regular user:

        #请执行
        mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config

        You should now deploy a pod network to the cluster.
        Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
        https://kubernetes.io/docs/concepts/cluster-administration/addons/

        You can now join any number of control-plane nodes by copying certificate authorities
        and service account keys on each node and then running the following as root:

        #这个是增加master节点命令 (去所需机器执行)
        kubeadm join 123.57.222.14:6443 --token qvmlnw.56mrs4l1rvobcfxh \
            --discovery-token-ca-cert-hash sha256:902c347eeffb6a7b7ae7265318f638db71a74bc474ceab3522fb2416c6794944 \
            --experimental-control-plane

        Then you can join any number of worker nodes by running the following on each as root:

        #这个是增加node节点命令 （去所需机器执行)
        kubeadm join 123.57.222.14:6443 --token qvmlnw.56mrs4l1rvobcfxh \
            --discovery-token-ca-cert-hash sha256:902c347eeffb6a7b7ae7265318f638db71a74bc474ceab3522fb2416c6794944


## 查看集群状态
    查看pod状态
        kubectl get pods --namespace=kube-system

        可以看到coredns没有启动，这是由于还没有配置网络插件，接下来配置下后再重新查看启动状态

##  安装网络组件
    wget https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
    kubectl apply -f kube-flannel.yml

## 再次查看集群coredns状态
    kubectl get pod -n kube-system

## 集群证书复制到master02,master03控制节点

    USER=root
    CONTROL_PLANE_IPS="10.0.1.208 10.0.1.209"
    for host in ${CONTROL_PLANE_IPS}; do
        scp /etc/kubernetes/pki/ca.crt "${USER}"@$host:
        scp /etc/kubernetes/pki/ca.key "${USER}"@$host:
        scp /etc/kubernetes/pki/sa.key "${USER}"@$host:
        scp /etc/kubernetes/pki/sa.pub "${USER}"@$host:
        scp /etc/kubernetes/pki/front-proxy-ca.crt "${USER}"@$host:
        scp /etc/kubernetes/pki/front-proxy-ca.key "${USER}"@$host:
        scp /etc/kubernetes/admin.conf "${USER}"@$host:
    done

## 登录master02、master03控制节点,移动证书到正确位置

    USER=root
    mkdir -p /etc/kubernetes/pki/etcd
    mv /${USER}/ca.crt /etc/kubernetes/pki/
    mv /${USER}/ca.key /etc/kubernetes/pki/
    mv /${USER}/sa.pub /etc/kubernetes/pki/
    mv /${USER}/sa.key /etc/kubernetes/pki/
    mv /${USER}/front-proxy-ca.crt /etc/kubernetes/pki/
    mv /${USER}/front-proxy-ca.key /etc/kubernetes/pki/
    mv /${USER}/admin.conf /etc/kubernetes/admin.conf


## 将master02、master03控制节点加入集群
    注意：加入集群的命令请使用master01节点安装完成后生生成的命令。
        kubeadm join 123.57.222.14:6443 --token qvmlnw.56mrs4l1rvobcfxh \
            --discovery-token-ca-cert-hash sha256:902c347eeffb6a7b7ae7265318f638db71a74bc474ceab3522fb2416c6794944 \
            --experimental-control-plane

        $ mkdir -p $HOME/.kube
        $ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        $ chown $(id -u):$(id -g) $HOME/.kube/config

## node节点加入加群
        kubeadm join 123.57.222.14:6443 --token qvmlnw.56mrs4l1rvobcfxh \
        --discovery-token-ca-cert-hash sha256:902c347eeffb6a7b7ae7265318f638db71a74bc474ceab3522fb2416c6794944
